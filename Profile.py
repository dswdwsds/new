import cloudscraper
from lxml import html
import json
import base64
import os
import requests
from urllib.parse import urlparse, parse_qs
from PIL import Image
from io import BytesIO

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª GitHub
access_token = os.getenv("ACCESS_TOKEN")
repo_name = "abdo12249/1"
repo_name_log = "abdo12249/test"
remote_path = "test1/animes.json"
missing_file_path = "missing_anime_log.json"
github_image_base = "https://abdo12249.github.io/1/images"

scraper = cloudscraper.create_scraper()

def extract_anime_id_from_custom_url(custom_url):
    query = parse_qs(urlparse(custom_url).query)
    return query.get("id", [None])[0]

def upload_image_to_github(image_bytes, path, commit_message):
    api_url = f"https://api.github.com/repos/{repo_name}/contents/{path}"
    headers = {"Authorization": f"token {access_token}"}

    sha = None
    check_response = scraper.get(api_url, headers=headers)
    if check_response.status_code == 200:
        try:
            sha = check_response.json().get("sha")
        except:
            pass

    encoded_content = base64.b64encode(image_bytes).decode()

    payload = {
        "message": commit_message,
        "content": encoded_content,
        "branch": "main"
    }
    if sha:
        payload["sha"] = sha

    put_response = scraper.put(api_url, headers=headers, json=payload)
    if put_response.status_code in [200, 201]:
        print(f"âœ… ØªÙ… Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ GitHub: {path}")
    else:
        print(f"âŒ ÙØ´Ù„ Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø©: {put_response.status_code} {put_response.text}")

def download_and_convert_to_webp(image_url, anime_id):
    try:
        response = requests.get(image_url, timeout=10)
        if response.status_code == 200:
            img = Image.open(BytesIO(response.content)).convert("RGB")
            buffer = BytesIO()
            img.save(buffer, "webp")
            buffer.seek(0)

            filename = f"{anime_id}.webp"
            github_path = f"images/{filename}"
            github_url = f"{github_image_base}/{filename}"

            upload_image_to_github(buffer.getvalue(), github_path, f"Ø±ÙØ¹ ØµÙˆØ±Ø© Ø§Ù„Ø£Ù†Ù…ÙŠ {anime_id}")

            return github_url
        else:
            print(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©: {image_url}")
            return ""
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù…ÙŠÙ„ Ø£Ùˆ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©: {e}")
        return ""

def fetch_anime_info(anime_id):
    anime_url = f"https://4d.qerxam.shop/anime/{anime_id}/"
    print(f"ğŸ“¥ ÙØªØ­ Ø§Ù„ØµÙØ­Ø©: {anime_url}")
    response = scraper.get(anime_url)
    if response.status_code != 200:
        print(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©: {anime_url}")
        return None

    tree = html.fromstring(response.content)

    def get_text(xpath):
        try:
            return tree.xpath(xpath)[0].text_content().strip()
        except:
            return ""

    def get_attr(xpath, attr):
        try:
            return tree.xpath(xpath)[0].attrib.get(attr, "").strip()
        except:
            return ""

    title = get_text("/html/body/div[2]/div/div/div[2]/div/h1")
    description = get_text("/html/body/div[2]/div/div/div[2]/div/p")
    original_image_url = get_attr("/html/body/div[2]/div/div/div[1]/div/img", "src")
    image_url = download_and_convert_to_webp(original_image_url, anime_id)
    tags = [tag.text_content().strip() for tag in tree.xpath("/html/body/div[2]/div/div/div[2]/div/ul/li") if tag.text_content().strip()]
    type_ = get_text("/html/body/div[2]/div/div/div[2]/div/div[1]/div[1]/div/a")
    status = get_text("/html/body/div[2]/div/div/div[2]/div/div[1]/div[3]/div/a")
    episode_count = get_text("/html/body/div[2]/div/div/div[2]/div/div[1]/div[4]/div")
    duration = get_text("/html/body/div[2]/div/div/div[2]/div/div[1]/div[5]/div")
    season = get_text("/html/body/div[2]/div/div/div[2]/div/div[1]/div[6]/div/a")
    source = get_text("/html/body/div[2]/div/div/div[2]/div/div[1]/div[7]/div")

    return {
        anime_id: {
            "title": title,
            "description": description,
            "image": image_url,
            "tags": tags,
            "type": type_,
            "status": status,
            "episodeCount": episode_count,
            "duration": duration,
            "season": season,
            "source": source
        }
    }

def upload_to_github(anime_data):
    api_url = f"https://api.github.com/repos/{repo_name}/contents/{remote_path}"
    headers = {"Authorization": f"token {access_token}"}

    response = scraper.get(api_url, headers=headers)
    current_data = {}
    sha = None

    if response.status_code == 200:
        try:
            sha = response.json()["sha"]
            content_decoded = base64.b64decode(response.json()["content"]).decode("utf-8")
            current_data = json.loads(content_decoded)
        except Exception as e:
            print("âš ï¸ ÙØ´Ù„ Ù‚Ø±Ø§Ø¡Ø© animes.json:", str(e))
    elif response.status_code == 404:
        print("ğŸ“ Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù animes.json Ø¬Ø¯ÙŠØ¯.")
    else:
        print("âŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ animes.json:", response.status_code)
        return

    updated = False
    for anime_id, info in anime_data.items():
        if anime_id not in current_data:
            print(f"â• Ø¥Ø¶Ø§ÙØ© Ø£Ù†Ù…ÙŠ Ø¬Ø¯ÙŠØ¯: {anime_id}")
            current_data[anime_id] = info
            updated = True
        else:
            print(f"â„¹ï¸ Ø§Ù„Ø£Ù†Ù…ÙŠ Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ù‹Ø§: {anime_id} (ØªÙ… Ø§Ù„ØªØ®Ø·ÙŠ)")

    if not updated:
        print("âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„Ø¥Ø¶Ø§ÙØªÙ‡Ø§.")
        return

    new_content = json.dumps(current_data, ensure_ascii=False, indent=2)
    encoded_content = base64.b64encode(new_content.encode()).decode()
import cloudscraper
from lxml import html
import json
import base64
import os
import requests
from urllib.parse import urlparse, parse_qs
from PIL import Image
from io import BytesIO

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª GitHub
access_token = os.getenv("ACCESS_TOKEN")
repo_name = "abdo12249/1"
repo_name_log = "abdo12249/test"
remote_path = "test1/animes.json"
missing_file_path = "missing_anime_log.json"
github_image_base = "https://abdo12249.github.io/1/images"

scraper = cloudscraper.create_scraper()

def extract_anime_id_from_custom_url(custom_url):
    query = parse_qs(urlparse(custom_url).query)
    return query.get("id", [None])[0]

def upload_image_to_github(image_bytes, path, commit_message):
    api_url = f"https://api.github.com/repos/{repo_name}/contents/{path}"
    headers = {"Authorization": f"token {access_token}"}

    sha = None
    check_response = scraper.get(api_url, headers=headers)
    if check_response.status_code == 200:
        try:
            sha = check_response.json().get("sha")
        except:
            pass

    encoded_content = base64.b64encode(image_bytes).decode()

    payload = {
        "message": commit_message,
        "content": encoded_content,
        "branch": "main"
    }
    if sha:
        payload["sha"] = sha

    put_response = scraper.put(api_url, headers=headers, json=payload)
    if put_response.status_code in [200, 201]:
        print(f"âœ… ØªÙ… Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ GitHub: {path}")
    else:
        print(f"âŒ ÙØ´Ù„ Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø©: {put_response.status_code} {put_response.text}")

def download_and_convert_to_webp(image_url, anime_id):
    try:
        response = requests.get(image_url, timeout=10)
        if response.status_code == 200:
            img = Image.open(BytesIO(response.content)).convert("RGB")
            buffer = BytesIO()
            img.save(buffer, "webp")
            buffer.seek(0)

            filename = f"{anime_id}.webp"
            github_path = f"images/{filename}"
            github_url = f"{github_image_base}/{filename}"

            upload_image_to_github(buffer.getvalue(), github_path, f"Ø±ÙØ¹ ØµÙˆØ±Ø© Ø§Ù„Ø£Ù†Ù…ÙŠ {anime_id}")

            return github_url
        else:
            print(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©: {image_url}")
            return ""
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù…ÙŠÙ„ Ø£Ùˆ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©: {e}")
        return ""

def fetch_anime_info(anime_id):
    anime_url = f"https://4d.qerxam.shop/anime/{anime_id}/"
    print(f"ğŸ“¥ ÙØªØ­ Ø§Ù„ØµÙØ­Ø©: {anime_url}")
    response = scraper.get(anime_url)
    if response.status_code != 200:
        print(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©: {anime_url}")
        return None

    tree = html.fromstring(response.content)

    def get_text(xpath):
        try:
            return tree.xpath(xpath)[0].text_content().strip()
        except:
            return ""

    def get_attr(xpath, attr):
        try:
            return tree.xpath(xpath)[0].attrib.get(attr, "").strip()
        except:
            return ""

    title = get_text("/html/body/div[2]/div/div/div[2]/div/h1")
    description = get_text("/html/body/div[2]/div/div/div[2]/div/p")
    original_image_url = get_attr("/html/body/div[2]/div/div/div[1]/div/img", "src")
    image_url = download_and_convert_to_webp(original_image_url, anime_id)
    tags = [tag.text_content().strip() for tag in tree.xpath("/html/body/div[2]/div/div/div[2]/div/ul/li") if tag.text_content().strip()]
    type_ = get_text("/html/body/div[2]/div/div/div[2]/div/div[1]/div[1]/div/a")
    status = get_text("/html/body/div[2]/div/div/div[2]/div/div[1]/div[3]/div/a")
    episode_count = get_text("/html/body/div[2]/div/div/div[2]/div/div[1]/div[4]/div")
    duration = get_text("/html/body/div[2]/div/div/div[2]/div/div[1]/div[5]/div")
    season = get_text("/html/body/div[2]/div/div/div[2]/div/div[1]/div[6]/div/a")
    source = get_text("/html/body/div[2]/div/div/div[2]/div/div[1]/div[7]/div")

    return {
        anime_id: {
            "title": title,
            "description": description,
            "image": image_url,
            "tags": tags,
            "type": type_,
            "status": status,
            "episodeCount": episode_count,
            "duration": duration,
            "season": season,
            "source": source
        }
    }

def upload_to_github(anime_data):
    api_url = f"https://api.github.com/repos/{repo_name}/contents/{remote_path}"
    headers = {"Authorization": f"token {access_token}"}

    response = scraper.get(api_url, headers=headers)
    current_data = {}
    sha = None

    if response.status_code == 200:
        try:
            sha = response.json()["sha"]
            content_decoded = base64.b64decode(response.json()["content"]).decode("utf-8")
            current_data = json.loads(content_decoded)
        except Exception as e:
            print("âš ï¸ ÙØ´Ù„ Ù‚Ø±Ø§Ø¡Ø© animes.json:", str(e))
    elif response.status_code == 404:
        print("ğŸ“ Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù animes.json Ø¬Ø¯ÙŠØ¯.")
    else:
        print("âŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ animes.json:", response.status_code)
        return

    updated = False
    for anime_id, info in anime_data.items():
        if anime_id not in current_data:
            print(f"â• Ø¥Ø¶Ø§ÙØ© Ø£Ù†Ù…ÙŠ Ø¬Ø¯ÙŠØ¯: {anime_id}")
            current_data[anime_id] = info
            updated = True
        else:
            print(f"â„¹ï¸ Ø§Ù„Ø£Ù†Ù…ÙŠ Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ù‹Ø§: {anime_id} (ØªÙ… Ø§Ù„ØªØ®Ø·ÙŠ)")

    if not updated:
        print("âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„Ø¥Ø¶Ø§ÙØªÙ‡Ø§.")
        return

    new_content = json.dumps(current_data, ensure_ascii=False, indent=2)
    encoded_content = base64.b64encode(new_content.encode()).decode()

    payload = {
        "message": "ØªØ­Ø¯ÙŠØ« animes.json Ø¨Ø¥Ø¶Ø§ÙØ© Ø£Ù†Ù…ÙŠ Ø¬Ø¯ÙŠØ¯",
        "content": encoded_content,
        "branch": "main"
    }
    if sha:
        payload["sha"] = sha

    put_response = scraper.put(api_url, headers=headers, json=payload)
    if put_response.status_code in [200, 201]:
        print("âœ… ØªÙ… Ø±ÙØ¹ animes.json Ø¨Ù†Ø¬Ø§Ø­.")
    else:
        print("âŒ ÙØ´Ù„ Ø±ÙØ¹ animes.json:", put_response.status_code, put_response.text)

# ========== Ø§Ù„ØªÙ†ÙÙŠØ° ==========

# ØªØ­Ù…ÙŠÙ„ missing_anime_log.json Ù…Ù† GitHub
missing_file_url = f"https://api.github.com/repos/{repo_name_log}/contents/{missing_file_path}"
headers = {"Authorization": f"token {access_token}"}

response = scraper.get(missing_file_url, headers=headers)
if response.status_code == 200:
    try:
        content_decoded = base64.b64decode(response.json()["content"]).decode("utf-8")
        missing_log = json.loads(content_decoded)
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ missing_anime_log.json Ù…Ù† GitHub Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        print("âŒ ÙØ´Ù„ ÙÙŠ ÙÙƒ ØªØ´ÙÙŠØ± Ø£Ùˆ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù missing_anime_log.json:", e)
        exit()
else:
    print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ missing_anime_log.json ÙÙŠ GitHub:", response.status_code)
    exit()

for entry in missing_log:
    custom_url = entry.get("episode_link")
    if not custom_url:
        continue

    anime_id = extract_anime_id_from_custom_url(custom_url)
    if not anime_id:
        print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ anime_id Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·:", custom_url)
        continue

    anime_info = fetch_anime_info(anime_id)
    if anime_info:
        upload_to_github(anime_info)
