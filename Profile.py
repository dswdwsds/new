# Profile.py
import cloudscraper
from lxml import html
import re
import json
import base64
import os
import time
from urllib.parse import urlparse, parse_qs, quote

# ----------- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª (Ø¹Ø¯Ù‘Ù„Ù‡Ø§ Ù„Ùˆ ØªØ­Ø¨) -----------
ACCESS_TOKEN = os.getenv("ACCESS_TOKEN")  # Ù„Ø§Ø²Ù… ÙŠÙƒÙˆÙ† PAT Ø¹Ù†Ø¯Ùƒ Ù„Ù„Ø±ÙØ¹ Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙŠØ¨Ùˆ Ø§Ù„Ù‡Ø¯Ù
INPUT_REPO = "abdo12249/test"             # Ù…ÙƒØ§Ù† missing_anime_log.json Ø§Ù„ØµØ­ÙŠØ­
INPUT_PATH = "missing_anime_log.json"
OUTPUT_REPO = "abdo12249/1"               # Ø§Ù„Ø±ÙŠØ¨Ùˆ Ø§Ù„Ù„ÙŠ Ù‡Ù†Ø±ÙØ¹ ÙÙŠÙ‡ animes.json
OUTPUT_PATH = "test1/animes1.json"
BRANCH = "main"
SLEEP_BETWEEN_FETCHES = 0.6               # Ù„ØªØ®ÙÙŠÙ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ±ÙØ±
# ------------------------------------------------

scraper = cloudscraper.create_scraper()
headers = {"Authorization": f"token {ACCESS_TOKEN}"} if ACCESS_TOKEN else {}

def fetch_file_from_github(repo, path):
    api_url = f"https://api.github.com/repos/{repo}/contents/{path}"
    resp = scraper.get(api_url, headers=headers)
    if resp.status_code == 200:
        j = resp.json()
        content_b64 = j.get("content", "")
        try:
            decoded = base64.b64decode(content_b64).decode("utf-8")
        except Exception as e:
            print("âŒ Ø®Ø·Ø£ ÙÙƒ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ø­ØªÙˆÙ‰:", e)
            return None, None
        return decoded, j.get("sha")
    elif resp.status_code == 404:
        print(f"âŒ 404 â€” Ø§Ù„Ù…Ù„Ù '{path}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ø±ÙŠØ¨Ùˆ '{repo}'.")
        return None, None
    else:
        print(f"âŒ Ø®Ø·Ø£ {resp.status_code} Ø¹Ù†Ø¯ Ø¬Ù„Ø¨ {repo}/{path}: {resp.text}")
        return None, None

def extract_anime_id_from_custom_link(link):
    try:
        query = parse_qs(urlparse(link).query)
        anime_id = query.get("id", [""])[0].strip()
        # Ø¥Ø²Ø§Ù„Ø© Ø±Ù‚Ù… Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ù…ØªÙˆØ§Ø¬Ø¯ ÙÙŠ Ø¢Ø®Ø± Ø§Ù„Ù€ id Ù…Ø«Ù„: name--2 Ø£Ùˆ name-2
        anime_id = re.sub(r'(?:--|-)\d+$', '', anime_id)
        return anime_id
    except Exception as e:
        print("âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø§Ø¨Ø·:", e)
        return ""

def fetch_anime_info_from_id(anime_id):
    slug = quote(anime_id, safe='')
    anime_url = f"https://4d.qerxam.shop/anime/{slug}/"
    print(f"ğŸ“¥ Ø¬Ù„Ø¨: {anime_url}")
    try:
        resp = scraper.get(anime_url)
    except Exception as e:
        print("âŒ Ø®Ø·Ø£ Ø·Ù„Ø¨ Ø§Ù„ØµÙØ­Ø©:", e)
        return None

    if resp.status_code != 200:
        print(f"âš ï¸ Ø§Ù„ØµÙØ­Ø© Ø±Ø¬Ø¹Øª status {resp.status_code} â€” ØªØ®Ø·ÙŠ {anime_id}")
        return None

    tree = html.fromstring(resp.content)

    def get_text(xpath):
        try:
            return tree.xpath(xpath)[0].text_content().strip()
        except:
            return ""

    def get_attr(xpath, attr):
        try:
            return tree.xpath(xpath)[0].attrib.get(attr, "").strip()
        except:
            return ""

    title = get_text('//div[@class="anime-details"]/h1')
    description = get_text('//div[@class="anime-details"]/p')
    image = get_attr('/html/body/div/div/div[1]/div/div/div[1]/div/img', 'src')
    tags = [tag.text_content().strip() for tag in tree.xpath('//div[@class="anime-details"]/ul/li') if tag.text_content().strip()]
    type_ = get_text('/html/body/div/div/div[1]/div/div/div[2]/div/div[1]/div[2]/div/a')
    status = get_text('/html/body/div/div/div[1]/div/div/div[2]/div/div[1]/div[4]/div/a')
    episode_count = get_text('/html/body/div/div/div[1]/div/div/div[2]/div/div[1]/div[5]/div/span')
    duration = get_text('/html/body/div/div/div[1]/div/div/div[2]/div/div[1]/div[6]/div')
    season = get_text('/html/body/div/div/div[1]/div/div/div[2]/div/div[1]/div[7]/div/a')
    source = get_text('/html/body/div/div/div[1]/div/div/div[2]/div/div[1]/div[10]/div')

    return {
        anime_id: {
            "title": title,
            "description": description,
            "image": image,
            "tags": tags,
            "type": type_,
            "status": status,
            "episodeCount": episode_count,
            "duration": duration,
            "season": season,
            "source": source
        }
    }

def merge_and_upload_batch(new_items):
    existing_raw, existing_sha = fetch_file_from_github(OUTPUT_REPO, OUTPUT_PATH)
    current_data = {}

    if existing_raw:
        try:
            current_data = json.loads(existing_raw)
        except Exception as e:
            print("âš ï¸ ÙØ´Ù„ Ù‚Ø±Ø§Ø¡Ø© animes.json Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© (Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙØ§Ø±Øº):", e)
            current_data = {}
    else:
        print("âš ï¸ Ù„Ù… Ø£Ø³ØªØ·Ø¹ ØªØ­Ù…ÙŠÙ„ animes.json Ø§Ù„Ù‚Ø¯ÙŠÙ… â€” Ù„Ù† Ø£Ø±ÙØ¹ Ø£ÙŠ ØªØ­Ø¯ÙŠØ« Ù„ØªØ¬Ù†Ø¨ Ø­Ø°Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        return  # <-- Ù…Ù‡Ù… Ø¬Ø¯Ù‹Ø§ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„ÙØ§Ø±ØºØ©

    added = 0
    for anime_id, info in new_items.items():
        if anime_id not in current_data:
            current_data[anime_id] = info
            added += 1
        else:
            print(f"â„¹ï¸ Ù…ØªÙˆØ§Ø¬Ø¯ Ù…Ø³Ø¨Ù‚Ù‹Ø§ØŒ ØªØ®Ø·ÙŠ: {anime_id}")

    if added == 0:
        print("âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ù†Ù…ÙŠØ§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„ÙŠØªÙ… Ø¥Ø¶Ø§ÙØªÙ‡Ø§.")
        return

    new_content = json.dumps(current_data, ensure_ascii=False, indent=2)
    encoded = base64.b64encode(new_content.encode()).decode()

    payload = {
        "message": f"ØªØ­Ø¯ÙŠØ« animes.json â€” Ø¥Ø¶Ø§ÙØ© {added} Ø£Ù†Ù…ÙŠØ§Øª",
        "content": encoded,
        "branch": BRANCH
    }
    if existing_sha:
        payload["sha"] = existing_sha

    put_url = f"https://api.github.com/repos/{OUTPUT_REPO}/contents/{OUTPUT_PATH}"
    resp = scraper.put(put_url, headers=headers, json=payload)
    if resp.status_code in (200, 201):
        print(f"âœ… ØªÙ… Ø±ÙØ¹ animes.json Ø¨Ù†Ø¬Ø§Ø­ â€” Ø£Ø¶ÙŠÙØª {added} Ø£Ù†Ù…ÙŠØ§Øª.")
    else:
        print("âŒ ÙØ´Ù„ Ø§Ù„Ø±ÙØ¹:", resp.status_code, resp.text)


def main():
    if not ACCESS_TOKEN:
        print("âš ï¸ ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… ØªÙˆÙÙŠØ± ACCESS_TOKEN Ø¹Ø¨Ø± Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø©. Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ù…Ù† Ø§Ù„Ø±ÙŠØ¨Ùˆ Ø§Ù„Ø¹Ø§Ù… Ù…Ù…ÙƒÙ†Ø©ØŒ Ù„ÙƒÙ† Ø§Ù„Ø±ÙØ¹ ÙŠØªØ·Ù„Ø¨ ØªÙˆÙƒÙ† Ù…Ø¹ ØµÙ„Ø§Ø­ÙŠØ© repo.")
    # Ø¬Ù„Ø¨ Ù…Ù„Ù missing_anime_log.json Ù…Ù† Ø§Ù„Ø±ÙŠØ¨Ùˆ Ø§Ù„ØµØ­ÙŠØ­
    raw, _ = fetch_file_from_github(INPUT_REPO, INPUT_PATH)
    if not raw:
        print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ missing_anime_log.json Ø£Ùˆ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„Ù‡.")
        return

    try:
        data = json.loads(raw)
    except Exception as e:
        print("âŒ ÙØ´Ù„ ØªØ­ÙˆÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ missing_anime_log.json Ø¥Ù„Ù‰ JSON:", e)
        return

    processed = set()
    collected = {}

    for entry in data:
        episode_link = entry.get("episode_link", "")
        anime_id = extract_anime_id_from_custom_link(episode_link)
        if not anime_id:
            print("âš ï¸ Ù„Ù… Ø§Ø³ØªØ®Ø±Ø¬ id Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·:", episode_link)
            continue
        if anime_id in processed:
            continue

        info = fetch_anime_info_from_id(anime_id)
        if info:
            collected.update(info)
            processed.add(anime_id)
        time.sleep(SLEEP_BETWEEN_FETCHES)

    if not collected:
        print("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ù†Ù…ÙŠØ§Øª ØµØ§Ù„Ø­Ø© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.")
        return

    # Ø±ÙØ¹ Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø© (Ø£Ø³Ø±Ø¹ ÙˆØ£ÙˆÙØ± Ù„Ø·Ù„Ø¨Ø§Øª API)
    merge_and_upload_batch(collected)
    print("ğŸ‰ Ø§Ù†ØªÙ‡Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.")

if __name__ == "__main__":
    main()
