import cloudscraper
from bs4 import BeautifulSoup
import time
import json
import re
from datetime import datetime
import os
import base64
import requests
from notifier import send_discord_notification

access_token = os.getenv("ACCESS_TOKEN")

repo_name = "abdo12249/1"
remote_folder = "test1/episodes"



repo_name_log = "abdo12249/test" # ÙŠÙ…ÙƒÙ† ØªØºÙŠÙŠØ± Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
missing_anime_log_filename = "missing_anime_log.json"

BASE_URL = "https://4i.nxdwle.shop"
EPISODE_LIST_URL = BASE_URL + "/episode/"
HEADERS = {"User-Agent": "Mozilla/5.0"}

scraper = cloudscraper.create_scraper()

def to_id_format(text):
text = text.strip().lower()
text = text.replace(":", "")
text = re.sub(r"[^a-z0-9()!- ]", "", text)
return text.replace(" ", "-")

def get_episode_links():
print("ğŸ“„ ØªØ­Ù…ÙŠÙ„ ØµÙØ­Ø© Ø§Ù„Ø­Ù„Ù‚Ø§Øª...")
response = scraper.get(EPISODE_LIST_URL, headers=HEADERS)
if response.status_code != 200:
print("âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©")
return []
soup = BeautifulSoup(response.text, "html.parser")
return [a.get("href") for a in soup.select(".episodes-card-title a") if a.get("href", "").startswith("http")]

def check_episode_on_github(anime_title):
anime_id = to_id_format(anime_title)
filename = anime_id + ".json"
url = f"https://api.github.com/repos/{repo_name}/contents/{remote_folder}/{filename}"
headers = {"Authorization": f"token {access_token}"}
response = scraper.get(url, headers=headers)
if response.status_code == 200:
download_url = response.json().get("download_url")
if download_url:
r = scraper.get(download_url)
if r.status_code == 200:
return True, r.json()
return True, None
elif response.status_code == 404:
return False, None
else:
return False, None

def get_episode_data(episode_url):
response = scraper.get(episode_url, headers=HEADERS)
if response.status_code != 200:
return None, None, None, None
soup = BeautifulSoup(response.text, "html.parser")
h3 = soup.select_one("div.main-section h3")
full_title = h3.get_text(strip=True) if h3 else "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
if "Ø§Ù„Ø­Ù„Ù‚Ø©" in full_title:
parts = full_title.rsplit("Ø§Ù„Ø­Ù„Ù‚Ø©", 1)
anime_title = parts[0].strip()
episode_number = parts[1].strip()
else:
anime_title = full_title
episode_number = "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
servers = []
for a in soup.select("ul#episode-servers li a"):
name = a.get_text(strip=True)
data_url = a.get("data-ep-url")
if isinstance(data_url, str):
url = data_url.strip()
if url.startswith("//"):
url = "https:" + url
servers.append({"serverName": name, "url": url})
return anime_title, episode_number, full_title, servers

def log_missing_anime(anime_title, episode_link):
"""
ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…ÙŠØ§Øª Ø§Ù„ØªÙŠ Ù„Ù… ÙŠÙƒÙ† Ù„Ù‡Ø§ Ù…Ù„Ù JSON Ù…ÙˆØ¬ÙˆØ¯ ÙˆØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ Ø­Ø¯ÙŠØ«Ù‹Ø§.
"""
api_url = f"https://api.github.com/repos/{repo_name_log}/contents/{missing_anime_log_filename}"
headers = {"Authorization": f"token {access_token}"}

# Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ Ù…Ù„Ù Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ  
response = scraper.get(api_url, headers=headers)  
log_data = []  
sha = None  

if response.status_code == 200:  
    sha = response.json().get("sha")  
    try:  
        content_decoded = base64.b64decode(response.json().get("content")).decode("utf-8")  
        log_data = json.loads(content_decoded)  
    except (json.JSONDecodeError, TypeError):  
        print("âš ï¸ ÙØ´Ù„ ÙÙƒ ØªØ´ÙÙŠØ± Ø£Ùˆ ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ. Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯.")  
        log_data = []  
elif response.status_code == 404:  
    print(f"â„¹ï¸ Ù…Ù„Ù Ø§Ù„Ø³Ø¬Ù„ {missing_anime_log_filename} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ Ø¹Ù„Ù‰ GitHub. Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡.")  
else:  
    print(f"âŒ ÙØ´Ù„ Ø¬Ù„Ø¨ Ù…Ù„Ù Ø§Ù„Ø³Ø¬Ù„ Ù…Ù† GitHub: {response.status_code} {response.text}")  

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§ Ø¨Ø§Ù„ÙØ¹Ù„  
new_entry = {  
    "anime_title": anime_title,  
    "episode_link": episode_link,  
    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")  
}  

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§ Ø¨Ø§Ù„ÙØ¹Ù„ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±  
# ÙŠÙ…ÙƒÙ† ØªØ¹Ø¯ÙŠÙ„ Ù‡Ø°Ø§ Ø§Ù„Ø´Ø±Ø· Ù„ÙŠÙƒÙˆÙ† Ø£ÙƒØ«Ø± ØªØ­Ø¯ÙŠØ¯Ù‹Ø§ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±  
if not any(item.get("anime_title") == anime_title and item.get("episode_link") == episode_link for item in log_data):  
    log_data.append(new_entry)  
    content_to_upload = json.dumps(log_data, indent=2, ensure_ascii=False)  
    encoded_content = base64.b64encode(content_to_upload.encode("utf-8")).decode()  

    payload = {  
        "message": f"ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ø£Ù†Ù…ÙŠØ§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©: Ø¥Ø¶Ø§ÙØ© {anime_title}",  
        "content": encoded_content,  
        "branch": "main"  
    }  
    if sha:  
        payload["sha"] = sha  

    r = scraper.put(api_url, headers=headers, json=payload)  
    if r.status_code in [200, 201]:  
        print(f"âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ø£Ù†Ù…ÙŠØ§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ÙÙŠ {missing_anime_log_filename} Ø¹Ù„Ù‰ GitHub.")  
    else:  
        print(f"âŒ ÙØ´Ù„ Ø±ÙØ¹ Ø³Ø¬Ù„ Ø§Ù„Ø£Ù†Ù…ÙŠØ§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¥Ù„Ù‰ GitHub: {r.status_code} {r.text}")  
else:  
    print(f"â„¹ï¸ Ø§Ù„Ø£Ù†Ù…ÙŠ '{anime_title}' Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ù„ÙØ¹Ù„ ÙÙŠ Ø³Ø¬Ù„ Ø§Ù„Ø£Ù†Ù…ÙŠØ§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©. ØªÙ… Ø§Ù„ØªØ®Ø·ÙŠ.")

def update_new_json_list(new_anime_filename):
"""
ØªØ­Ø¯ÙŠØ« Ù…Ù„Ù Ø§Ù„Ø¬Ø¯ÙŠØ¯.json Ø¹Ù†Ø¯ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯ Ù„Ø£Ù†Ù…ÙŠ.
"""
new_json_url = f"https://abdo12249.github.io/1/test1/episodes/{new_anime_filename}"
api_url = f"https://api.github.com/repos/{repo_name}/contents/test1/Ø§Ù„Ø¬Ø¯ÙŠØ¯.json"
headers = {"Authorization": f"token {access_token}"}

# Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø­Ø§Ù„ÙŠ  
response = scraper.get(api_url, headers=headers)  
sha = None  
data = {"animes": []}  

if response.status_code == 200:  
    sha = response.json().get("sha")  
    try:  
        content_decoded = base64.b64decode(response.json().get("content")).decode("utf-8")  
        data = json.loads(content_decoded)  
    except Exception as e:  
        print("âš ï¸ ÙØ´Ù„ Ù‚Ø±Ø§Ø¡Ø© Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¬Ø¯ÙŠØ¯.json:", str(e))  
else:  
    print("ğŸ“ Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„Ø¬Ø¯ÙŠØ¯.json Ø¬Ø¯ÙŠØ¯.")  

# ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø±Ø§Ø¨Ø· Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±  
if new_json_url not in data["animes"]:  
    data["animes"].append(new_json_url)  
    content_to_upload = json.dumps(data, indent=2, ensure_ascii=False)  
    encoded_content = base64.b64encode(content_to_upload.encode()).decode()  

    payload = {  
        "message": f"ØªØ­Ø¯ÙŠØ« Ù…Ù„Ù Ø§Ù„Ø¬Ø¯ÙŠØ¯.json Ø¨Ø¥Ø¶Ø§ÙØ© {new_anime_filename}",  
        "content": encoded_content,  
        "branch": "main"  
    }  
    if sha:  
        payload["sha"] = sha  

    r = scraper.put(api_url, headers=headers, json=payload)  
    if r.status_code in [200, 201]:  
        print("ğŸ“„ ØªÙ… ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯.json âœ…")  
    else:  
        print("âŒ ÙØ´Ù„ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯.json:", r.status_code, r.text)  
else:  
    print("â„¹ï¸ Ø§Ù„Ø±Ø§Ø¨Ø· Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ù‹Ø§ ÙÙŠ Ø§Ù„Ø¬Ø¯ÙŠØ¯.jsonØŒ ØªÙ… Ø§Ù„ØªØ®Ø·ÙŠ.")

def save_to_json(anime_title, episode_number, episode_title, servers):
anime_id = to_id_format(anime_title)
filename = anime_id + ".json"
api_url = f"https://api.github.com/repos/{repo_name}/contents/{remote_folder}/{filename}"
headers = {"Authorization": f"token {access_token}"}
exists_on_github, github_data = check_episode_on_github(anime_title)

ep_data = {  
    "number": int(episode_number) if episode_number.isdigit() else episode_number,  
    "title": f"Ø§Ù„Ø­Ù„Ù‚Ø© {episode_number}",  
    "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),  
    "link": f"https://abdo12249.github.io/1/test1/Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ù‡.html?id={anime_id}&episode={episode_number}",  
    "image": f"https://abdo12249.github.io/1/images/{anime_id}.webp",  
    "servers": servers  
}  

if not exists_on_github:  
    print(f"ğŸš€ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯ Ù„Ù„Ø£Ù†Ù…ÙŠ: {filename}")  
    new_data = {  
        "animeTitle": anime_title,  
        "episodes": [ep_data]  
    }  
    content = json.dumps(new_data, indent=2, ensure_ascii=False)  
    encoded = base64.b64encode(content.encode()).decode()  
    payload = {  
        "message": f"Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù {filename} Ù…Ø¹ Ø§Ù„Ø­Ù„Ù‚Ø© {episode_number}",  
        "content": encoded,  
        "branch": "main"  
    }  
    r = scraper.put(api_url, headers=headers, json=payload)  
    if r.status_code in [200, 201]:  
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù ÙˆØ±ÙØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ GitHub.")  
        send_discord_notification(anime_title, episode_number, ep_data["link"], ep_data["image"])  
        log_missing_anime(anime_title, ep_data["link"])  
        update_new_json_list(filename)  # âœ… Ù‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù‡Ù…  
    else:  
        print(f"âŒ ÙØ´Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù Ø¹Ù„Ù‰ GitHub: {r.status_code} {r.text}")  
    return  


if github_data is None:  
    print("âš ï¸ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù Ù…Ù† GitHub.")  
    return  

updated = False  
found = False  
for i, ep in enumerate(github_data["episodes"]):  
    if str(ep["number"]) == str(ep_data["number"]):  
        found = True  
        if ep["servers"] != ep_data["servers"]:  
            github_data["episodes"][i] = ep_data  
            updated = True  
            print(f"ğŸ”„ ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ù„Ù‚Ø© {episode_number} Ù„Ø£Ù† Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª ØªØºÙŠØ±Øª.")  
            send_discord_notification(anime_title, episode_number, ep_data["link"], ep_data["image"])  
        else:  
            print(f"âš ï¸ Ø§Ù„Ø­Ù„Ù‚Ø© {episode_number} Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ù†ÙØ³ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ØªÙ… ØªØ®Ø·ÙŠÙ‡Ø§.")  
        break  
if not found:  
    github_data["episodes"].append(ep_data)  
    updated = True  
    print(f"â• ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø­Ù„Ù‚Ø© {episode_number} Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©.")  
    send_discord_notification(anime_title, episode_number, ep_data["link"], ep_data["image"])  
if updated:  
    content = json.dumps(github_data, indent=2, ensure_ascii=False)  
    encoded = base64.b64encode(content.encode()).decode()  
    sha_response = scraper.get(api_url, headers=headers)  
    sha = sha_response.json().get("sha") if sha_response.status_code == 200 else None  
    payload = {  
        "message": f"ØªØ­Ø¯ÙŠØ« {filename} - Ø§Ù„Ø­Ù„Ù‚Ø© {episode_number}",  
        "content": encoded,  
        "branch": "main"  
    }  
    if sha:  
        payload["sha"] = sha  
    r = scraper.put(api_url, headers=headers, json=payload)  
    if r.status_code in [200, 201]:  
        print(f"ğŸš€ ØªÙ… Ø±ÙØ¹ Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¥Ù„Ù‰ GitHub Ø¨Ù†Ø¬Ø§Ø­.")  
    else:  
        print(f"âŒ ÙØ´Ù„ Ø±ÙØ¹ Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¥Ù„Ù‰ GitHub: {r.status_code} {r.text}")



all_links = get_episode_links()

for idx, link in enumerate(all_links):
print(f"\nğŸ”¢ Ø­Ù„Ù‚Ø© {idx+1}/{len(all_links)}")
anime_name, episode_number, full_title, server_list = get_episode_data(link)
if anime_name and server_list:
save_to_json(anime_name, episode_number, full_title, server_list)
else:
print("âŒ ØªØ®Ø·ÙŠØª Ø§Ù„Ø­Ù„Ù‚Ø© Ø¨Ø³Ø¨Ø¨ Ø®Ø·Ø£.")
time.sleep(1)
